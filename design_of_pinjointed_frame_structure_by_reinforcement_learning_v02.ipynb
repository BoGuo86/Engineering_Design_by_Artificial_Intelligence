{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAUTION: this code has serious memory leaks. To perform training you need at least 4GB RAM (at least 1GB should be free). To have adequate model performance you need to re-run the notebook (training) for at least several times, i.e. \"Kernel-Restart&Run All\" for at least several times or you can use pretrained \"pinjointed1.ckpt\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model tries to optimize the position of middle element by allowing its movement along horizontal axis in order to have displacements in the structure within acceptable limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more details on the model check \"Engineering design of \n",
    "# 1D rod and 2D pin-jointed frame structure driven by reinforcement \n",
    "# learning and finite element analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre- and post-processing of FE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from shutil import copyfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH0=\"C:\\\\Temp\\\\test0.dat\"\n",
    "PATH=\"C:\\\\Temp\\\\test.dat\"\n",
    "PATH1=\"C:\\\\Temp\\\\test.res\"\n",
    "\n",
    "PATH2 =\"C:\\\\Temp\\\\test1.dat\"\n",
    "PATH3 =\"C:\\\\Temp\\\\test1.res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_dat(path):\n",
    "    \n",
    "    # nels - the number of elements\n",
    "    # nn  - total number of nodes in problem\n",
    "    # ndim - number of dimensions\n",
    "    # np_types - number of different property types\n",
    "    # prop - element properties matrix\n",
    "    # g_coord - nodal coordinates\n",
    "    # g_num  - element connectivity\n",
    "    # nr - number of restraints\n",
    "    # nf - nodal freedom array\n",
    "    # loaded_nodes - the number of nodes with forces applied\n",
    "    # loads\n",
    "    # fixed_freedoms - number of fixed freedoms\n",
    "\n",
    "    f=open(path,\"r\")\n",
    "    nels,nn,ndim,np_types=np.array(f.readline().split()).astype(int)\n",
    "    prop=float(f.readline())\n",
    "    g_coord=[]\n",
    "    for i in range(math.ceil(nn/3)):\n",
    "        g_coord+=f.readline().split()\n",
    "    g_coord=np.array(g_coord).astype(float)\n",
    "    g_num =[]\n",
    "    for i in range(math.ceil(nels/5)):\n",
    "        g_num +=f.readline().split()\n",
    "    g_num =np.array(g_num ).astype(int)\n",
    "\n",
    "    nr=int(f.readline())\n",
    "    nf=np.array(f.readline().split()).astype(int)\n",
    "    loaded_nodes =int(f.readline())\n",
    "    loads=np.array(f.readline().split()).astype(float) # loads[0] should be type int\n",
    "    fixed_freedoms=int(f.readline())\n",
    "    f.close()  \n",
    "    return nels,nn,ndim,np_types,prop,g_coord,g_num,nr,nf,loaded_nodes,loads,fixed_freedoms  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nels,nn,ndim,np_types,prop,g_coord,g_num,nr,nf,loaded_nodes,loads,fixed_freedoms=read_dat(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insert_element_coord(g_coord,nn):\n",
    "#     g_coord_=g_coord.reshape(nn,2)\n",
    "#     min_x=g_coord_.min(axis=0)[0]\n",
    "#     max_x=g_coord_.max(axis=0)[0]\n",
    "#     min_y=g_coord_.min(axis=0)[1]\n",
    "#     max_y=g_coord_.max(axis=0)[1]\n",
    "#     new_el_x=(max_x-min_x)/2\n",
    "#     new_el_y=max_y-min_y\n",
    "#     for i in range(0,len(g_coord),2):\n",
    "#         if g_coord[i]<new_el_x:\n",
    "#             ins=i      \n",
    "#     a=g_coord.tolist()\n",
    "#     a.insert(ins+2,new_el_x) \n",
    "#     a.insert(ins+3,0) \n",
    "#     a.insert(ins+4,new_el_x) \n",
    "#     a.insert(ins+5,new_el_y) \n",
    "#     return np.array(a)       \n",
    "                   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert_element_coord(g_coord,nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alter_dat(action,path,fl):\n",
    "    dx=0.1\n",
    "    nels,nn,ndim,np_types,prop,g_coord,g_num,nr,nf,loaded_nodes,loads,fixed_freedoms=read_dat(path)\n",
    "    flag=0\n",
    "    \n",
    "    if fl==0:\n",
    "        if action==0: \n",
    "            g_coord[4]=g_coord[4]-dx\n",
    "        elif action==1:  \n",
    "            g_coord[4]=g_coord[4]+dx\n",
    "        elif action==2:  \n",
    "            g_coord[8]=g_coord[8]-dx \n",
    "        elif action==3:\n",
    "            g_coord[8]=g_coord[8]+dx\n",
    "        elif action==4:\n",
    "            g_coord=g_coord\n",
    "    elif fl==1:        \n",
    "        g_coord=np.array([ 0.,3.,4.,0.,5.,3.,8.,3.,5.,0.,12.,0.])\n",
    "    elif fl==2:    \n",
    "        g_coord=np.array([ 0.,3.,4.,0.,5.,3.,10.,3.,5.,0.,15.,0.])\n",
    "        \n",
    "    if (g_coord[8]>g_coord[10]-2*dx)or(g_coord[8]<g_coord[3]+2*dx):\n",
    "        flag=1\n",
    "        return flag, g_coord \n",
    "        \n",
    "    if (g_coord[4]>g_coord[6]-2*dx)or(g_coord[4]<g_coord[0]+2*dx):  \n",
    "        flag=1\n",
    "        return flag, g_coord \n",
    "                \n",
    "    f=open(path, \"r\") \n",
    "    all_lines=f.readlines() \n",
    "    f.close()\n",
    "\n",
    "    for j in range(len(all_lines)):\n",
    "        all_lines[j]=all_lines[j].rstrip(\"\\n\") \n",
    "    \n",
    "    f=open(path, \"w\")   \n",
    "    for i in range(len(all_lines)):\n",
    "        if i ==2:\n",
    "            f.writelines(\"{} {}  {} {}  {} {}\\n\".format(g_coord[0],g_coord[1],g_coord[2],g_coord[3],g_coord[4],g_coord[5]))\n",
    "            f.writelines(\"{} {}  {} {}  {} {}\\n\".format(g_coord[6],g_coord[7],g_coord[8],g_coord[9],g_coord[10],g_coord[11]))\n",
    "        elif i!=3:\n",
    "            f.writelines(all_lines[i]+\"\\n\")\n",
    "    f.close() \n",
    "\n",
    "    return flag, g_coord     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alter_dat(3,PATH,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_res(path):\n",
    "    f=open(path, \"r\")\n",
    "    all_lines=f.readlines() \n",
    "    f.close()\n",
    "\n",
    "    A=np.zeros(shape=(6,2))\n",
    "    A[0][0]=all_lines[3].split()[1]\n",
    "    A[0][1]=all_lines[3].split()[2]\n",
    "    A[1][0]=all_lines[4].split()[1]\n",
    "    A[1][1]=all_lines[4].split()[2]\n",
    "    A[2][0]=all_lines[5].split()[1]\n",
    "    A[2][1]=all_lines[5].split()[2]\n",
    "    A[3][0]=all_lines[6].split()[1]\n",
    "    A[3][1]=all_lines[6].split()[2]\n",
    "    A[4][0]=all_lines[7].split()[1]\n",
    "    A[4][1]=all_lines[7].split()[2]\n",
    "    A[5][0]=all_lines[8].split()[1]\n",
    "    A[5][1]=all_lines[8].split()[2]\n",
    "    return A    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abs(read_res(PATH1)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(abs(read_res(PATH1).reshape(12,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def copy_file(path0,path):\n",
    "#     copyfile(path0,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe(coord,l1,l2,res):    \n",
    "    return coord[4]/l1,coord[8]/l2, res.reshape(12,)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Element Model of pin-jointed frame structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FE model is taken from \"Programming the finite element method, I. M. Smith et al, 5th edition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FEA(path):\n",
    "    target=path.split(\".\")\n",
    "    p=subprocess.Popen(\"C:\\\\Temp\\\\p42.exe {}\".format(target[0]), \\\n",
    "                       stdout=subprocess.PIPE, shell=False, stderr=subprocess.STDOUT)\n",
    "    p.wait()\n",
    "#     return p.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEA(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Policy - Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details of model can be found in the book:\n",
    "# Hands-On Machine Learning with Scikit-Learn & TensorFlow. Aurйlien Gйron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 3 \n",
    "n_hidden = 50 \n",
    "n_outputs = 5 \n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Build the neural network\n",
    "X_ = tf.placeholder(tf.float64, shape=[None, n_inputs], name=\"X_\")\n",
    "hidden = fully_connected(X_, n_hidden, activation_fn=tf.nn.elu, weights_initializer=initializer)\n",
    "hidden1 = fully_connected(hidden, n_hidden, activation_fn=tf.nn.elu, weights_initializer=initializer)\n",
    "logits = fully_connected(hidden1, n_outputs, activation_fn=None, weights_initializer=initializer)\n",
    "outputs = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "# Select a random action based on the estimated probabilities\n",
    "action = tf.multinomial(tf.log(outputs), num_samples=1,output_dtype=tf.int32)\n",
    "\n",
    "y=tf.reshape(tf.one_hot(action,depth=5,dtype=tf.float64),[5,1])\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=tf.transpose(logits))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(xentropy)\n",
    "gradients = [grad for grad, variable in grads_and_vars]\n",
    "gradient_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "for grad, variable in grads_and_vars:\n",
    "    gradient_placeholder = tf.placeholder(tf.float64, shape=grad.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "\n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate=0.97):\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(all_rewards, discount_rate=0.97):\n",
    "    all_discounted_rewards = [discount_rewards(rewards) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_(obs_,obs):\n",
    "    \n",
    "    if abs(obs_[2])>abs(obs[2]):  \n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tracemalloc\n",
    "# import linecache\n",
    "\n",
    "# def display_top(snapshot, key_type='lineno', limit=3):\n",
    "#     snapshot = snapshot.filter_traces((\n",
    "#         tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "#         tracemalloc.Filter(False, \"<unknown>\"),\n",
    "#     ))\n",
    "#     top_stats = snapshot.statistics(key_type)\n",
    "\n",
    "#     print(\"Top %s lines\" % limit)\n",
    "#     for index, stat in enumerate(top_stats[:limit], 1):\n",
    "#         frame = stat.traceback[0]\n",
    "#         # replace \"/path/to/module/file.py\" with \"module/file.py\"\n",
    "#         filename = os.sep.join(frame.filename.split(os.sep)[-2:])\n",
    "#         print(\"#%s: %s:%s: %.1f KiB\"\n",
    "#               % (index, filename, frame.lineno, stat.size / 1024))\n",
    "#         line = linecache.getline(frame.filename, frame.lineno).strip()\n",
    "#         if line:\n",
    "#             print('    %s' % line)\n",
    "\n",
    "#     other = top_stats[limit:]\n",
    "#     if other:\n",
    "#         size = sum(stat.size for stat in other)\n",
    "#         print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\n",
    "#     total = sum(stat.size for stat in top_stats)\n",
    "#     print(\"Total allocated size: %.1f KiB\" % (total / 1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./policy1/pinjointed1.ckpt\n",
      "Saving 0 iteration\n",
      "Saving 5 iteration\n",
      "Saving 10 iteration\n",
      "Saving 15 iteration\n",
      "Saving 20 iteration\n"
     ]
    }
   ],
   "source": [
    "n_iterations =21 #250 # number of training iterations\n",
    "n_max_steps = 300 #1000 # max steps per episode\n",
    "n_games_per_update = 10 # train the policy every 10 episodes\n",
    "save_iterations = 5 # save the model every 10 training iterations\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    init.run()      \n",
    "    saver.restore(sess, \"./policy1/pinjointed1.ckpt\")\n",
    "    for iteration in range(n_iterations):\n",
    "            \n",
    "        all_rewards = [] # all sequences of raw rewards for each episode\n",
    "        all_gradients = [] # gradients saved at each step of each episode\n",
    "             \n",
    "        for game in range(n_games_per_update):\n",
    "            current_rewards = [] # all raw rewards from the current episode\n",
    "            current_gradients = [] # all gradients from the current episode\n",
    "            \n",
    "            flag, g_coord=alter_dat(4,PATH,1)\n",
    "            FEA(PATH)\n",
    "            res=read_res(PATH1)\n",
    "            obs=observe(g_coord,8,8,res)\n",
    "            \n",
    "#             tracemalloc.start()################################################\n",
    "            for step in range(n_max_steps):\n",
    "                action_val, gradients_val = sess.run([action, gradients],\n",
    "                                                     feed_dict={X_: np.array(obs).reshape(1,n_inputs)}) \n",
    "                obs_=obs\n",
    "                flag, g_coord=alter_dat(action_val[0][0],PATH,0)\n",
    "                if flag==1: \n",
    "                    break\n",
    "                    \n",
    "                FEA(PATH)\n",
    "                res=read_res(PATH1)\n",
    "                obs=observe(g_coord,8,8,res)               \n",
    "                reward=reward_(obs_,obs)\n",
    "                \n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "#             snapshot = tracemalloc.take_snapshot()#################################\n",
    "#             display_top(snapshot)#####################################\n",
    "        \n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "\n",
    "    \n",
    "            \n",
    "        # At this point we have run the policy for 10 episodes, and we are\n",
    "        # ready for a policy update using the algorithm described earlier.\n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards)\n",
    "        \n",
    "        feed_dict = {}\n",
    "        for var_index, grad_placeholder in enumerate(gradient_placeholders):\n",
    "            # multiply the gradients by the action scores, and compute the mean\n",
    "            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index] \n",
    "                                      for game_index, rewards in enumerate(all_rewards)\n",
    "                                      for step, reward in enumerate(rewards)],axis=0)\n",
    "            feed_dict[grad_placeholder] = mean_gradients\n",
    "        \n",
    "        \n",
    "        sess.run(training_op, feed_dict=feed_dict)\n",
    "        \n",
    "        if iteration % save_iterations == 0:\n",
    "            print(\"Saving {} iteration\".format(iteration))\n",
    "            saver.save(sess, \"./policy1/pinjointed1.ckpt\")\n",
    "\n",
    "end=time.time()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3398.6333904266357\n"
     ]
    }
   ],
   "source": [
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(path,path1,op,l1,l2):\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.import_meta_graph('./policy1/pinjointed1.ckpt.meta')\n",
    "        saver.restore(sess, \"./policy1/pinjointed1.ckpt\") \n",
    "\n",
    "        graph = tf.get_default_graph()\n",
    "        outputs = graph.get_tensor_by_name(\"Y_proba:0\") \n",
    "        X_ = graph.get_tensor_by_name(\"X_:0\") \n",
    "        \n",
    "        flag, g_coord=alter_dat(4,path,op)\n",
    "        FEA(path)\n",
    "        res=read_res(path1)\n",
    "        obs=observe(g_coord,l1,l2,res)\n",
    "        \n",
    "        for step in range(50):\n",
    "            action_val= sess.run([outputs],feed_dict={X_: np.array(obs).reshape(1,n_inputs)})\n",
    "            flag, g_coord=alter_dat(np.argmax(action_val),path,0)\n",
    "#             print(np.argmax(action_val))\n",
    "            if flag==1: \n",
    "                break  \n",
    "            FEA(path)\n",
    "            res=read_res(path1)\n",
    "            obs=observe(g_coord,l1,l2,res)\n",
    "        \n",
    "        return obs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./policy1/pinjointed1.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.625, 0.4500000000000004, -0.006606)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(PATH,PATH1,1,8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check \"test.dat\" to see how the geometry of the structure has changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./policy1/pinjointed1.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5, 0.363636363636364, -0.01368)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(PATH2,PATH3,2,10,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check \"test1.dat\" to see how the geometry of the new structure has changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
